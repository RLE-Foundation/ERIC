{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3869d1e2-440b-45b7-a2e1-3cdadba95629",
   "metadata": {},
   "source": [
    "### LIBERO: A Benchmark for Lifelong Robotic Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6bb4f1-8e2c-4796-9ad4-7f9e1da68821",
   "metadata": {},
   "source": [
    "[LIBERO (Lifelong Independence, Benchmarking, and reproducibility in Robot Learning)](https://libero-project.github.io) is a comprehensive benchmark designed to evaluate and accelerate research in lifelong learning for robot manipulation. It provides a suite of tasks and a framework for assessing an agent's ability to acquire new skills and knowledge over time, without forgetting previously learned abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e047c4a-a3e5-45b5-af5a-7df852e6ce8a",
   "metadata": {},
   "source": [
    "#### Part 1: What is LIBERO?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d216e-2b96-46d1-ac10-fb2bc8781516",
   "metadata": {
    "tags": []
   },
   "source": [
    "The core philosophy of LIBERO is to facilitate the study of knowledge transfer in robotic agents. This includes the transfer of both declarative knowledge (what objects are and their properties) and procedural knowledge (how to perform actions and manipulations). The environment is built upon a modular and extensible task generation pipeline, allowing for the creation of a diverse and ever-growing set of challenges for robotic agents.\n",
    "\n",
    "Key features of the LIBERO environment include:\n",
    "\n",
    "- **A Suite of Diverse Tasks**: LIBERO offers a collection of manipulation tasks with varying objects, initial conditions, and goals. These tasks are designed to test different aspects of lifelong learning, including forward transfer (learning new tasks faster), backward transfer (improving performance on old tasks), and resistance to catastrophic forgetting.\n",
    "\n",
    "- **Procedural Task Generation**: The environment includes tools for procedurally generating new tasks, ensuring a continuous stream of novel challenges for lifelong learning agents.\n",
    "\n",
    "- **Standardized Evaluation Metrics**: LIBERO provides a consistent set of metrics for evaluating the performance of lifelong learning algorithms, enabling fair and reproducible comparisons between different approaches.\n",
    "\n",
    "- **Focus on Realistic Scenarios**: The tasks in LIBERO are inspired by real-world manipulation challenges, pushing the research towards more practical and general-purpose robotic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb085966-299e-4234-bbfe-d15c0efe0d0d",
   "metadata": {},
   "source": [
    "#### Part 2: Trying out LIBERO: A Getting Started Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a474b-b65d-4631-8a7e-1ad47c98a79a",
   "metadata": {},
   "source": [
    "To get started with the LIBERO environment, you will need to clone the official GitHub repository and install the necessary dependencies. The following steps will guide you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c649f-5766-4311-a84a-1a848dd28e9e",
   "metadata": {},
   "source": [
    "**Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf4c2a0-9b7c-4769-b3b8-ed5f06ccfe05",
   "metadata": {
    "tags": []
   },
   "source": [
    "To get started with the LIBERO environment, you will need to clone the official GitHub repository and install the necessary dependencies. The following steps will guide you through the process.\n",
    "```bash\n",
    "# 1. Create conda environment\n",
    "conda create -n libero python=3.8.13\n",
    "conda activate libero\n",
    "\n",
    "# 2. Install LIBERO dependencies\n",
    "git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git\n",
    "cd LIBERO\n",
    "pip install -r requirements.txt\n",
    "pip install torch==1.11.0+cu113 torch|vision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "```\n",
    "Then install the `libero` package:\n",
    "```bash\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aad1bf-380a-449b-8826-90eb16af2c37",
   "metadata": {},
   "source": [
    "**Download the Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e701f0-1211-490b-8fc5-3170ff405464",
   "metadata": {},
   "source": [
    "LIBERO offers high-quality human teleoperation demonstrations across four distinct task suites: `libero_spatial`, `libero_object`, `libero_100`, and `libero_goal`.\n",
    "\n",
    "To download a specific dataset, use the following command:\n",
    "\n",
    "```bash\n",
    "python benchmark_scripts/download_libero_datasets.py --datasets DATASET\n",
    "```\n",
    "\n",
    "Replace `DATASET` with your desired choice from the four options listed above. The datasets are saved in the `LIBERO` folder by default.\n",
    "\n",
    "If you'd like to download all four datasets at once, simply omit the `--datasets` parameter:\n",
    "\n",
    "```bash\n",
    "python benchmark_scripts/download_libero_datasets.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057baa37-37f6-44a5-a7ba-d15598d4a016",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Trying it out**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6508bb2-dabd-4704-a5d9-4f70675e3227",
   "metadata": {},
   "source": [
    "- **Retrieving a Task**: Here's a minimal example of how to retrieve a specific task from a task suite.\n",
    "```python\n",
    "from libero.libero import benchmark\n",
    "from libero.libero.envs import OffScreenRenderEnv\n",
    "\n",
    "\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "task_suite_name = \"libero_10\" # can also choose libero_spatial, libero_object, etc.\n",
    "task_suite = benchmark_dict[task_suite_name]()\n",
    "\n",
    "# retrieve a specific task\n",
    "task_id = 0\n",
    "task = task_suite.get_task(task_id)\n",
    "task_name = task.name\n",
    "task_description = task.language\n",
    "task_bddl_file = os.path.join(get_libero_path(\"bddl_files\"), task.problem_folder, task.bddl_file)\n",
    "print(f\"[info] retrieving task {task_id} from suite {task_suite_name}, the \" + \\\n",
    "      f\"language instruction is {task_description}, and the bddl file is {task_bddl_file}\")\n",
    "\n",
    "# step over the environment\n",
    "env_args = {\n",
    "    \"bddl_file_name\": task_bddl_file,\n",
    "    \"camera_heights\": 128,\n",
    "    \"camera_widths\": 128\n",
    "}\n",
    "env = OffScreenRenderEnv(**env_args)\n",
    "env.seed(0)\n",
    "env.reset()\n",
    "init_states = task_suite.get_task_init_states(task_id) # for benchmarking purpose, we fix the a set of initial states\n",
    "init_state_id = 0\n",
    "env.set_init_state(init_states[init_state_id])\n",
    "\n",
    "dummy_action = [0.] * 7\n",
    "for step in range(10):\n",
    "    obs, reward, done, info = env.step(dummy_action)\n",
    "env.close()\n",
    "```\n",
    "\n",
    "- **Training**: Currently, LIBERO mainly focuses on **lifelong imitation learning**. To begin a lifelong learning experiment, first select your desired `BENCHMARK`, `POLICY`, and `ALGO`:\n",
    "\n",
    "    - `BENCHMARK` from: `[LIBERO_SPATIAL, LIBERO_OBJECT, LIBERO_GOAL, LIBERO_90, LIBERO_10]`\n",
    "\n",
    "    - `POLICY` from: `[bc_rnn_policy, bc_transformer_policy, bc_vilt_policy]`\n",
    "\n",
    "    - `ALGO` from: `[base, er, ewc, packnet, multitask]`\n",
    "    \n",
    "Then, execute the following command:\n",
    "```bash\n",
    "export CUDA_VISIBLE_DEVICES=GPU_ID && \\\n",
    "export MUJOCO_EGL_DEVICE_ID=GPU_ID && \\\n",
    "python libero/lifelong/main.py seed=SEED \\\n",
    "                               benchmark_name=BENCHMARK \\\n",
    "                               policy=POLICY \\\n",
    "                               lifelong=ALGO\n",
    "```\n",
    "For detailed information on reproducing study results, please refer to the official documentation.\n",
    "\n",
    "- **Evaluation**: By default, policies are evaluated during the training process. For users with limited GPU resources, a separate evaluation script is available.\n",
    "```bash\n",
    "python libero/lifelong/evaluate.py --benchmark BENCHMARK_NAME \\\n",
    "                                   --task_id TASK_ID \\\n",
    "                                   --algo ALGO_NAME \\\n",
    "                                   --policy POLICY_NAME \\\n",
    "                                   --seed SEED \\\n",
    "                                   --ep EPOCH \\\n",
    "                                   --load_task LOAD_TASK \\\n",
    "                                   --device_id CUDA_ID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2823d-8335-4543-af0c-a10a226058e7",
   "metadata": {},
   "source": [
    "### OpenVLA: An Open-Source Vision-Language-Action Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa99ec-e704-4b40-841e-a9b3dfc6507c",
   "metadata": {},
   "source": [
    "[OpenVLA](https://openvla.github.io/) is a powerful, open-source Vision-Language-Action (VLA) model designed for robotic manipulation. It leverages the capabilities of large pre-trained models to enable robots to understand natural language instructions and perform a wide range of tasks based on visual input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88006263-16e9-4321-962f-ab559b3536b8",
   "metadata": {},
   "source": [
    "#### Part 1: What is OpenVLA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae28192-f177-402b-9a38-2608ac5811e3",
   "metadata": {},
   "source": [
    "OpenVLA is built upon a foundation of leading-edge models, including the Prismatic-7B VLM (Vision Language Model), which itself integrates SigLIP and DinoV2 for visual understanding and a Llama 2-based language model. This architecture allows OpenVLA to effectively ground language commands in visual scenes and translate them into executable robotic actions.\n",
    "\n",
    "The model was trained on the extensive Open X-Embodiment dataset, a large-scale, multi-platform dataset of robot trajectories. This diverse training data endows OpenVLA with the ability to generalize to new objects, tasks, and environments that it has not encountered during training.\n",
    "\n",
    "Key features of the OpenVLA model include:\n",
    "\n",
    "- **Natural Language Instruction Following**: OpenVLA can interpret complex, high-level commands given in natural language and execute the corresponding actions.\n",
    "\n",
    "- **Strong Generalization Capabilities**: Thanks to its training on a massive and diverse dataset, the model can adapt to novel scenarios and instructions.\n",
    "\n",
    "- **Open-Source and Accessible**: As an open-source project, OpenVLA provides researchers and developers with access to the model weights, code, and pre-trained checkpoints, fostering collaboration and innovation in the robotics community.\n",
    "\n",
    "- **Real-time Performance**: The model is designed to be efficient enough for real-time control of robotic hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c7469-d9b1-47ec-9b8c-7c9d28fa789a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Part 2: Trying out OpenVLA: A Getting Started Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f4b2b-c80d-4cf0-ad1b-cf5fb094a995",
   "metadata": {},
   "source": [
    "This repository was built with `Python 3.10` but should be compatible with any version of `Python >= 3.8`, and it requires `PyTorch 2.2.*`. The latest version of this repository has been thoroughly tested and developed with the following dependencies:\n",
    "- `PyTorch`: 2.2.0\n",
    "- `torchvision`: 0.17.0\n",
    "- `transformers`: 4.40.1\n",
    "- `tokenizers`: 0.19.1\n",
    "- `timm`: 0.9.10\n",
    "- `flash-attn`: 2.5.5\n",
    "\n",
    "**Installation**\n",
    "\n",
    "To get started, use the setup commands below:\n",
    "```bash\n",
    "# Create conda environment\n",
    "conda create -n openvla python=3.10 -y\n",
    "conda activate openvla\n",
    "\n",
    "# Install PyTorch. Below is a sample command to do this, but you should check the following link\n",
    "# to find installation instructions that are specific to your compute platform:\n",
    "# https://pytorch.org/get-started/locally/\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia -y\n",
    "\n",
    "# Clone and install the openvla repo\n",
    "git clone https://github.com/openvla/openvla.git\n",
    "cd openvla\n",
    "pip install -e .\n",
    "\n",
    "# Install Flash Attention 2 for training (https://github.com/Dao-AILab/flash-attention)\n",
    "#   =>> If you run into difficulty, try `pip cache remove flash_attn` first\n",
    "pip install packaging ninja\n",
    "ninja --version; echo $?  # Verify Ninja --> should return exit code \"0\"\n",
    "pip install \"flash-attn==2.5.5\" --no-build-isolation\n",
    "```\n",
    "\n",
    "**Pretrained VLAs**\n",
    "\n",
    "Two OpenVLA models trained with checkpoints, configs, and model cards available on HuggingFace page.\n",
    "\n",
    "- [`openvla-7b`](https://huggingface.co/openvla/openvla-7b): Trained from the Prismatic prism-dinosiglip-224px VLM (based on a fused DINOv2 and SigLIP vision backbone, and Llama-2 LLM). Trained on a large mixture of datasets from Open X-Embodiment spanning 970K trajectories.\n",
    "\n",
    "- ['openvla-v01-7b'](https://huggingface.co/openvla/openvla-v01-7b): An early model used during development, trained from the Prismatic siglip-224px VLM (singular SigLIP vision backbone, and a Vicuña v1.5 LLM). Trained on the same mixture of datasets as Octo, but for significantly fewer GPU hours than our final model.\n",
    "\n",
    "**Fine-Tuning OpenVLA via LoRA**\n",
    "\n",
    "Using **Low-Rank Adaptation (LoRA)** via the Hugging Face `transformers` library is a highly effective way to fine-tune a 7B-parameter model. The main script for LoRA fine-tuning is `vla-scripts/finetune.py`. Once the dataset is downloaded, you can launch the fine-tuning script. Here is an example:\n",
    "\n",
    "```bash\n",
    "torchrun --standalone --nnodes 1 --nproc-per-node 1 vla-scripts/finetune.py \\\n",
    "  --vla_path \"openvla/openvla-7b\" \\\n",
    "  --data_root_dir <PATH TO BASE DATASETS DIR> \\\n",
    "  --dataset_name bridge_orig \\\n",
    "  --run_root_dir <PATH TO LOG/CHECKPOINT DIR> \\\n",
    "  --adapter_tmp_dir <PATH TO TEMPORARY DIR TO SAVE ADAPTER WEIGHTS> \\\n",
    "  --lora_rank 32 \\\n",
    "  --batch_size 1 \\\n",
    "  --grad_accumulation_steps 4 \\\n",
    "  --learning_rate 5e-4 \\\n",
    "  --image_aug <True or False> \\\n",
    "  --wandb_project <PROJECT> \\\n",
    "  --wandb_entity <ENTITY> \\\n",
    "  --save_steps <NUMBER OF GRADIENT STEPS PER CHECKPOINT SAVE>\n",
    "```\n",
    "\n",
    "For smaller GPUs, simply reduce `--batch_size` and increase `--grad_accumulation_steps` to maintain a sufficiently large effective batch size for stable training. When `grad_accumulation_steps` is greater than 1, the model accumulates gradients over multiple batches before performing a single parameter update. This simulates training with a much larger batch size, which is crucial for stable training with limited VRAM. For instance, setting `grad_accumulation_steps=4` means the model updates its parameters only once every four batches. \n",
    "\n",
    "If you have multiple GPUs, you can use PyTorch Distributed Data Parallel (DDP) by setting `--nproc-per-node` to the number of available GPUs in the `torchrun` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd6379-08ee-4751-a680-93728ca03711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
