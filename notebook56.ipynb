{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO) Algorithm: Theory and Implementation\n",
    "\n",
    "This notebook provides a comprehensive introduction to the Proximal Policy Optimization (PPO) algorithm, combining mathematical foundations with practical implementation for Atari environments. PPO is one of the most successful policy gradient methods, known for its stability and sample efficiency.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "PPO addresses the challenge of policy gradient methods by introducing a **clipped surrogate objective** that prevents destructively large policy updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Configuration\n",
    "\n",
    "PPO requires careful environment configuration and hyperparameter setup. Let's examine the key components from our Atari implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    # Environment settings\n",
    "    env_id: str = \"BreakoutNoFrameskip-v4\"\n",
    "    total_timesteps: int = 10000000\n",
    "    \n",
    "    # PPO hyperparameters\n",
    "    learning_rate: float = 2.5e-4\n",
    "    num_envs: int = 8\n",
    "    num_steps: int = 128\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_coef: float = 0.1\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    \n",
    "    # Training configuration\n",
    "    update_epochs: int = 4\n",
    "    num_minibatches: int = 4\n",
    "    norm_adv: bool = True\n",
    "    clip_vloss: bool = True\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Hyperparameters Explained:**\n",
    "- `clip_coef` (ε): Controls the clipping range for the policy update\n",
    "- `gamma` (γ): Discount factor for future rewards\n",
    "- `gae_lambda` (λ): Controls bias-variance tradeoff in advantage estimation\n",
    "- `ent_coef`: Entropy regularization coefficient to encourage exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment preprocessing for Atari\n",
    "def make_env(env_id, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        \n",
    "        # Standard Atari preprocessing\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "        # ...\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayScaleObservation(env)\n",
    "        env = gym.wrappers.FrameStack(env, 4)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Neural Network Architecture (Actor-Critic)\n",
    "\n",
    "PPO uses an Actor-Critic architecture where both policy and value function share feature extraction layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        # Shared feature extraction network for 84x84x4 Atari frames\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64 * 7 * 7, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Actor head: outputs action probabilities π(a|s)\n",
    "        self.actor = layer_init(nn.Linear(512, envs.single_action_space.n), std=0.01)\n",
    "        \n",
    "        # Critic head: outputs state value V(s)\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        \"\"\"Get state value V(s)\"\"\"\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        \"\"\"Core forward pass: returns action, log_prob, entropy, value\"\"\"\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture Components:**\n",
    "- **Shared CNN**: Processes visual observations (4 stacked frames)\n",
    "- **Actor Network**: Outputs policy π(a|s) as categorical distribution\n",
    "- **Critic Network**: Estimates state value V(s) for advantage computation\n",
    "- **Orthogonal Initialization**: Improves training stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Trajectory Collection and Rollout\n",
    "\n",
    "PPO collects trajectories through environment interaction before updating the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage setup for rollout data\n",
    "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "# Rollout loop\n",
    "for step in range(0, args.num_steps):\n",
    "    global_step += args.num_envs\n",
    "    obs[step] = next_obs\n",
    "    dones[step] = next_done\n",
    "\n",
    "    # Action selection using current policy\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "        values[step] = value.flatten()\n",
    "    \n",
    "    actions[step] = action\n",
    "    logprobs[step] = logprob\n",
    "\n",
    "    # Execute action in environment\n",
    "    next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "    next_done = np.logical_or(terminations, truncations)\n",
    "    rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "    next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rollout Process:**\n",
    "1. **State Storage**: Store current observations and terminal flags\n",
    "2. **Action Selection**: Sample actions from current policy π_θ(a|s)\n",
    "3. **Environment Step**: Execute actions and collect rewards\n",
    "4. **Data Storage**: Store (s_t, a_t, r_t, log π(a_t|s_t), V(s_t)) for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advantage Estimation with GAE\n",
    "\n",
    "Generalized Advantage Estimation (GAE) is crucial for reducing variance in policy gradient estimates.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "GAE computes advantages using:\n",
    "$$\\hat{A}_t^{GAE(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+l}^V$$\n",
    "\n",
    "Where the TD error is:\n",
    "$$\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap value if not done\n",
    "with torch.no_grad():\n",
    "    next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "    advantages = torch.zeros_like(rewards).to(device)\n",
    "    lastgaelam = 0\n",
    "    \n",
    "    # GAE computation (backward pass)\n",
    "    for t in reversed(range(args.num_steps)):\n",
    "        if t == args.num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - dones[t + 1]\n",
    "            nextvalues = values[t + 1]\n",
    "        \n",
    "        # TD error: δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "        delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "        \n",
    "        # GAE recursion: A_t = δ_t + γλA_{t+1}\n",
    "        advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "    \n",
    "    # Returns for value function training\n",
    "    returns = advantages + values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GAE Benefits:**\n",
    "- **Variance Reduction**: Reduces variance compared to Monte Carlo estimates\n",
    "- **Bias Control**: λ parameter controls bias-variance tradeoff\n",
    "- **Stability**: Leads to more stable policy updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: PPO Loss Functions and Optimization\n",
    "\n",
    "The core of PPO lies in its clipped surrogate objective that prevents destructively large policy updates.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "PPO uses a clipped surrogate objective:\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "Where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten batch data\n",
    "b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "b_logprobs = logprobs.reshape(-1)\n",
    "b_actions = actions.reshape(-1)\n",
    "b_advantages = advantages.reshape(-1)\n",
    "b_returns = returns.reshape(-1)\n",
    "b_values = values.reshape(-1)\n",
    "\n",
    "# Optimization loop\n",
    "b_inds = np.arange(args.batch_size)\n",
    "clipfracs = []\n",
    "\n",
    "for epoch in range(args.update_epochs):\n",
    "    np.random.shuffle(b_inds)\n",
    "    for start in range(0, args.batch_size, args.minibatch_size):\n",
    "        end = start + args.minibatch_size\n",
    "        mb_inds = b_inds[start:end]\n",
    "\n",
    "        # Re-evaluate actions under current policy\n",
    "        _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n",
    "            b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "        )\n",
    "        \n",
    "        # Compute probability ratio r_t = π_new(a|s) / π_old(a|s)\n",
    "        logratio = newlogprob - b_logprobs[mb_inds]\n",
    "        ratio = logratio.exp()\n",
    "\n",
    "        # Approximate KL divergence for monitoring\n",
    "        with torch.no_grad():\n",
    "            approx_kl = ((ratio - 1) - logratio).mean()\n",
    "            clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "        # Advantage normalization\n",
    "        mb_advantages = b_advantages[mb_inds]\n",
    "        if args.norm_adv:\n",
    "            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "        # === PPO CLIPPED OBJECTIVE ===\n",
    "        pg_loss1 = -mb_advantages * ratio\n",
    "        pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "        # === VALUE FUNCTION LOSS ===\n",
    "        newvalue = newvalue.view(-1)\n",
    "        if args.clip_vloss:\n",
    "            v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "            v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                newvalue - b_values[mb_inds], -args.clip_coef, args.clip_coef\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "            v_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()\n",
    "        else:\n",
    "            v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "        # === TOTAL LOSS ===\n",
    "        entropy_loss = entropy.mean()\n",
    "        loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "        # Gradient update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Early stopping based on KL divergence\n",
    "    if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Components:**\n",
    "\n",
    "1. **Policy Loss**: $L^{CLIP}(\\theta)$ - Clipped surrogate objective\n",
    "2. **Value Loss**: $L^{VF}(\\theta) = \\mathbb{E}[(V_\\theta(s_t) - V_t^{targ})^2]$\n",
    "3. **Entropy Loss**: $S[\\pi_\\theta](s_t)$ - Encourages exploration\n",
    "\n",
    "**Combined Objective:**\n",
    "$$L(\\theta) = L^{CLIP}(\\theta) + c_1 L^{VF}(\\theta) - c_2 S[\\pi_\\theta](s_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate annealing\n",
    "if args.anneal_lr:\n",
    "    frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "    lrnow = frac * args.learning_rate\n",
    "    optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "# Logging metrics\n",
    "writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Why PPO Works\n",
    "\n",
    "**Key Advantages:**\n",
    "1. **Stability**: Clipping prevents destructively large policy updates\n",
    "2. **Sample Efficiency**: Multiple epochs per batch of collected data\n",
    "3. **Simplicity**: Relatively simple to implement and tune\n",
    "4. **Robustness**: Works well across diverse environments\n",
    "\n",
    "**Critical Components:**\n",
    "- **Clipped Objective**: Ensures conservative policy updates\n",
    "- **GAE**: Reduces variance in advantage estimation\n",
    "- **Actor-Critic**: Shared features improve sample efficiency\n",
    "- **Multiple Epochs**: Maximizes data utilization\n",
    "\n",
    "**Monitoring Metrics:**\n",
    "- **KL Divergence**: Should stay below ~0.05 for stability\n",
    "- **Clip Fraction**: Indicates how often clipping is triggered\n",
    "- **Value Loss**: Measures quality of value function approximation\n",
    "- **Entropy**: Tracks exploration vs exploitation balance\n",
    "\n",
    "PPO has become the gold standard for policy gradient methods due to its excellent balance of performance, stability, and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO) Algorithm with VLA Implementation\n",
    "\n",
    "This character provides a comprehensive introduction to the Proximal Policy Optimization (PPO) algorithm, combining mathematical foundations with practical implementation using Vision-Language-Action (VLA) models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Dependencies\n",
    "\n",
    "PPO requires careful environment configuration and dependency management. Let's examine the key imports and setup from our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration for robotics simulation\n",
    "import os\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "# ... other environment variables\n",
    "\n",
    "# Core libraries for PPO\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Components:**\n",
    "- **Environment Variables**: Configure rendering and distributed training\n",
    "- **LIBERO Environment**: Robotics simulation framework\n",
    "- **VLA Models**: Vision-Language-Action transformers for policy representation\n",
    "- **Distributed Training**: Multi-GPU support for scalable training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: PPO Agent Architecture and Neural Networks\n",
    "\n",
    "The core of PPO lies in its actor-critic architecture. Our implementation uses a VLA-based agent with separate value head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, vla_base, action_tokenizer, processor, device):\n",
    "        super().__init__()\n",
    "        self.vla_base = vla_base\n",
    "        self.action_tokenizer = action_tokenizer\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.config = vla_base.module.config.text_config\n",
    "        \n",
    "        # Value head for critic network\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(self.config.n_embd, 1024, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1, bias=False),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, input_ids, attention_mask, pixel_values, **kwargs):\n",
    "        transformer_outputs = self.vla_base(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            pixel_values=pixel_values,\n",
    "            output_hidden_states=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        hidden_states = transformer_outputs.hidden_states[-1][:, -1, :].float()\n",
    "        values = self.value_head(hidden_states)\n",
    "        return values\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematical Foundation:**\n",
    "\n",
    "The agent implements both policy $\\pi_\\theta(a|s)$ and value function $V^\\pi(s)$:\n",
    "\n",
    "- **Policy Network**: $\\pi_\\theta(a|s) = \\text{VLA}(\\text{prompt}, \\text{image})$\n",
    "- **Value Network**: $V^\\pi(s) = \\text{ValueHead}(\\text{hidden\\_states})$\n",
    "\n",
    "Where the VLA model processes both visual observations and language instructions to generate action tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Trajectory Collection and Rollout Process\n",
    "\n",
    "PPO collects trajectories through environment interaction. Here's how our implementation handles the rollout phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollout phase - collect trajectories\n",
    "if distributed_state.is_main_process:\n",
    "    print(\"Rollout...\")\n",
    "    for step in tqdm(range(0, args.num_steps), desc=\"Rollout Progress\"):\n",
    "        global_step += args.num_envs\n",
    "        \n",
    "        # Encode inputs for the agent\n",
    "        query_inputs = agent.process_to_inputs(envs.prompt, obs_img, args.max_seq_len)\n",
    "        \n",
    "        # Action selection using current policy\n",
    "        with torch.no_grad():\n",
    "            action, value, logprob, sequences = agent.act_rollout(query_inputs)\n",
    "            values[step, :] = value.flatten()\n",
    "        \n",
    "        # Store trajectory data\n",
    "        traj_sequences[step, :] = sequences\n",
    "        traj_attention_mask[step, :] = query_inputs['attention_mask']\n",
    "        traj_obs[step, :] = query_inputs['pixel_values'].to(device)\n",
    "        dones[step, :] = next_done\n",
    "        logprobs[step, :] = logprob\n",
    "\n",
    "        # Execute action in environment\n",
    "        obs_img, reward, done, infos = envs.step(action[0])\n",
    "        rewards[step, :] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_done = torch.Tensor([done]).to(device)\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Overview:**\n",
    "1. **State Encoding**: Convert visual observations and language prompts to model inputs\n",
    "2. **Action Sampling**: Use current policy to sample actions and compute log probabilities\n",
    "3. **Environment Step**: Execute actions and collect rewards\n",
    "4. **Data Storage**: Store $(s_t, a_t, r_t, \\log \\pi_\\theta(a_t|s_t), V(s_t))$ for later updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advantage Estimation and GAE Computation\n",
    "\n",
    "Generalized Advantage Estimation (GAE) is crucial for stable PPO training. Here's the mathematical implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized Advantage Estimation (GAE)\n",
    "with torch.no_grad():\n",
    "    next_value = torch.zeros(1, 1).to(device)\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(args.num_steps)):\n",
    "        if t == args.num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - dones[t + 1]\n",
    "            nextvalues = values[t + 1]\n",
    "        \n",
    "        # Temporal difference error\n",
    "        delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "        \n",
    "        # GAE computation\n",
    "        advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "    \n",
    "    # Returns for value function training\n",
    "    returns = advantages + values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematical Foundation:**\n",
    "\n",
    "GAE computes advantages using a weighted sum of temporal difference errors:\n",
    "\n",
    "$$\\hat{A}_t^{GAE(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+l}^V$$\n",
    "\n",
    "Where:\n",
    "- $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error\n",
    "- $\\gamma$ is the discount factor\n",
    "- $\\lambda$ controls the bias-variance tradeoff\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces variance compared to Monte Carlo estimates\n",
    "- Maintains low bias with appropriate $\\lambda$ values\n",
    "- Improves training stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Policy Optimization and Loss Functions\n",
    "\n",
    "The core PPO update involves the clipped surrogate objective and value function loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Update Loop\n",
    "for epoch in range(args.update_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Re-evaluate actions under current policy\n",
    "        newlogprob, newvalue = agent.encode_traj(\n",
    "            mb_sequences, mb_attention_mask, mb_pixel_values\n",
    "        )\n",
    "        \n",
    "        # Compute probability ratio\n",
    "        logratio = newlogprob - mb_logprobs\n",
    "        ratio = logratio.exp()\n",
    "        \n",
    "        # Advantage normalization\n",
    "        if args.norm_adv:\n",
    "            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "        # Clipped surrogate objective (Policy Loss)\n",
    "        pg_loss1 = -mb_advantages * ratio\n",
    "        pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "        # Value function loss\n",
    "        if args.clip_vloss:\n",
    "            v_loss_unclipped = (newvalue - mb_returns) ** 2\n",
    "            v_clipped = mb_values + torch.clamp(\n",
    "                newvalue - mb_values, -args.clip_coef, args.clip_coef\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - mb_returns) ** 2\n",
    "            v_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()\n",
    "        else:\n",
    "            v_loss = 0.5 * ((newvalue - mb_returns) ** 2).mean()\n",
    "\n",
    "        # Total loss\n",
    "        loss = pg_loss + v_loss * args.vf_coef\n",
    "        \n",
    "        # Gradient update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematical Foundation:**\n",
    "\n",
    "PPO uses a clipped surrogate objective to prevent large policy updates:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio\n",
    "- $\\hat{A}_t$ is the advantage estimate\n",
    "- $\\epsilon$ is the clipping parameter (typically 0.1 or 0.2)\n",
    "\n",
    "**Value Function Loss:**\n",
    "$$L^{VF}(\\theta) = \\mathbb{E}_t\\left[(V_\\theta(s_t) - V_t^{targ})^2\\right]$$\n",
    "\n",
    "**Combined Objective:**\n",
    "$$L(\\theta) = L^{CLIP}(\\theta) + c_1 L^{VF}(\\theta)$$\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Clipping**: Prevents destructively large policy updates\n",
    "- **Stability**: More stable than vanilla policy gradients\n",
    "- **Sample Efficiency**: Reuses data through multiple epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This implementation demonstrates PPO's effectiveness for training VLA models in robotics tasks. Key advantages include:\n",
    "\n",
    "1. **Stable Training**: Clipped objectives prevent destructive updates\n",
    "2. **Sample Efficiency**: Multiple epochs per batch of data\n",
    "3. **Scalability**: Distributed training support\n",
    "4. **Flexibility**: Works with complex vision-language-action models\n",
    "\n",
    "The combination of PPO with VLA models opens new possibilities for training embodied AI agents that can understand natural language instructions and perform complex manipulation tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
 
}